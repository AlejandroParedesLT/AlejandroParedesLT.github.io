<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Explainable AI on Alejandro Paredes La Torre</title>
    <link>http://localhost:1313/categories/explainable-ai/</link>
    <description>Recent content in Explainable AI on Alejandro Paredes La Torre</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/explainable-ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Unmasking AI: Technical Insights and Policy Implications</title>
      <link>http://localhost:1313/posts/unmaskingai/</link>
      <pubDate>Sun, 16 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/unmaskingai/</guid>
      <description>&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&#xD;&#xA;# Understanding Bias in AI: A Technical Exploration&#xD;&#xA;&#xD;&#xA;Joy Buolamwini’s *Unmasking AI* illustrates the intersection of machine learning, human-centered design, and policy. This post highlights the book’s key technical lessons and their implications for AI governance. Interactive elements allow readers to engage with concepts like dataset composition, classification errors, and intersectional performance disparities.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;## Interactive Chapter Guide&#xD;&#xA;&#xD;&#xA;Click to expand technical and policy insights from each chapter.&#xD;&#xA;&#xD;&#xA;&amp;lt;details&amp;gt;&#xD;&#xA;&amp;lt;summary&amp;gt;Chapter 1–3: Foundations and Early Observations&amp;lt;/summary&amp;gt;&#xD;&#xA;&#xD;&#xA;**Technical Takeaways**:&#xD;&#xA;&#xD;&#xA;* Early exposure to computing, graphics, and human-centered design.&#xD;&#xA;* Experiments with interactive installations revealed bias in computer vision systems (e.g., Upbeat Walls failing for darker faces).&#xD;&#xA;&#xD;&#xA;**Policy Insight**:&#xD;&#xA;&#xD;&#xA;* Even early-stage tech environments can embed exclusionary practices.&#xD;&#xA;* Raises awareness that human-centered AI design must consider diverse populations.&#xD;&#xA;&#xD;&#xA;&amp;lt;/details&amp;gt;&#xD;&#xA;&#xD;&#xA;&amp;lt;details&amp;gt;&#xD;&#xA;&amp;lt;summary&amp;gt;Chapter 4–6: Public Demonstrations and Face Analytics&amp;lt;/summary&amp;gt;&#xD;&#xA;&#xD;&#xA;**Technical Takeaways**:&#xD;&#xA;&#xD;&#xA;* Aspire Mirror exhibit demonstrates face-detection failures.&#xD;&#xA;* Defines “face analytics” tasks: detection, classification, recognition.&#xD;&#xA;* Highlights “AI functionality fallacy”: overestimating what algorithms can do.&#xD;&#xA;&#xD;&#xA;**Interactive Python Example: Simulating Detection Bias**&#xD;&#xA;&#xD;&#xA;```python&#xD;&#xA;import matplotlib.pyplot as plt&#xD;&#xA;import numpy as np&#xD;&#xA;&#xD;&#xA;# Simulate misclassification probabilities&#xD;&#xA;skin_tones = [&amp;#39;Light&amp;#39;, &amp;#39;Medium&amp;#39;, &amp;#39;Dark&amp;#39;]&#xD;&#xA;genders = [&amp;#39;Male&amp;#39;, &amp;#39;Female&amp;#39;]&#xD;&#xA;error_rates = np.array([[0.02, 0.10], [0.05, 0.20], [0.08, 0.30]])  # Dark female highest&#xD;&#xA;&#xD;&#xA;fig, ax = plt.subplots()&#xD;&#xA;im = ax.imshow(error_rates, cmap=&amp;#39;Reds&amp;#39;)&#xD;&#xA;&#xD;&#xA;# Show labels&#xD;&#xA;ax.set_xticks(np.arange(len(genders)))&#xD;&#xA;ax.set_yticks(np.arange(len(skin_tones)))&#xD;&#xA;ax.set_xticklabels(genders)&#xD;&#xA;ax.set_yticklabels(skin_tones)&#xD;&#xA;ax.set_title(&amp;#34;Simulated Face Recognition Error Rates&amp;#34;)&#xD;&#xA;for i in range(len(skin_tones)):&#xD;&#xA;    for j in range(len(genders)):&#xD;&#xA;        ax.text(j, i, f&amp;#34;{error_rates[i, j]*100:.0f}%&amp;#34;, ha=&amp;#34;center&amp;#34;, va=&amp;#34;center&amp;#34;, color=&amp;#34;black&amp;#34;)&#xD;&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This plot mirrors Buolamwini’s findings: intersectional analysis is critical to uncover hidden bias.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding SHAP Values: From Theory to Implementation</title>
      <link>http://localhost:1313/posts/shapely-values/</link>
      <pubDate>Sun, 19 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/shapely-values/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; — SHAP (SHapley Additive exPlanations) is a principled framework for interpreting ML predictions. By borrowing ideas from cooperative game theory, SHAP quantifies the contribution of each feature for a given prediction. In this guide, we’ll unpack the math, intuition, and practical implementation.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;img src=&#34;pizza_shapley_story.gif&#34; alt=&#34;Pizza Shapley Animation&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-motivation--why-interpretability-matters&#34;&gt;1. Motivation — Why Interpretability Matters&lt;/h2&gt;&#xA;&lt;p&gt;Modern machine learning models are increasingly complex, think ensembles, gradient boosting, or deep neural networks. Without interpretability, these models are &lt;strong&gt;black boxes&lt;/strong&gt;. Understanding &lt;em&gt;why&lt;/em&gt; a model makes a prediction is essential for:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
