<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>

<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
    
    
    <title>Alejandro Paredes La Torre</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="">
    <meta name="generator" content="Hugo 0.151.2">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >




    


    
      

    

    
    
      <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Alejandro Paredes La Torre" />
      <link href="/index.xml" rel="feed" type="application/rss+xml" title="Alejandro Paredes La Torre" />
      
    

    
      <link rel="canonical" href="http://localhost:1313/">
    

    <meta property="og:url" content="http://localhost:1313/">
  <meta property="og:site_name" content="Alejandro Paredes La Torre">
  <meta property="og:title" content="Alejandro Paredes La Torre">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="website">

  <meta itemprop="name" content="Alejandro Paredes La Torre">
  <meta itemprop="datePublished" content="2025-11-16T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-11-16T00:00:00+00:00">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Alejandro Paredes La Torre">

	
  </head><body class="ma0 avenir bg-near-white development">

    

  <header>
    <div class="pb3-m pb6-l bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Alejandro Paredes La Torre
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv3 ph3 ph4-ns">
        <h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">
          Alejandro Paredes La Torre
        </h1>
        
      </div>
    </div>
  </header>


    <main class="pb7" role="main">
      
  <article class="cf ph3 ph5-l pv3 pv4-l f4 tc-l center measure-wide lh-copy nested-links mid-gray">
    
  </article>

  
  
  
  
  

  
    <div class="pa3 pa4-ns w-100 w-70-ns center">
      

      <section class="w-100 mw8">
        
        
          <div class="w-100 mb4 relative">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l dark-gray no-underline">
    <div class="flex-column flex-row-ns flex">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/posts/ahackersmind/" class="color-inherit dim link">
            Applying Concepts from A Hacker&#39;s Mind by Bruce Schneier
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <h1 id="a-hackers-mind-real-world-applied-insights">A Hacker&rsquo;s Mind: Real World Applied Insights</h1>
<p><strong>A Hacker&rsquo;s Mind</strong> analyses how hacking transcends code, networks, and cryptographic systems. Schneier frames hacking as exploiting rules, incentives, and institutional structures. This post examines direct applications from the book through modern examples and thought experiments.</p>
<hr>
<h2 id="core-premise-illustrated">Core Premise Illustrated</h2>
<p>Rules are not objective barriers. They are design artifacts. Anyone with incentive, creativity, and domain insight can search for unintended outcomes inside procedural, legal, economic, or algorithmic frameworks.</p>
        </div>
        <a href="/posts/ahackersmind/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
      </div>
    </div>
  </div>
</article>

          </div>
        
          <div class="w-100 mb4 relative">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l dark-gray no-underline">
    <div class="flex-column flex-row-ns flex">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/posts/shapely-values/" class="color-inherit dim link">
            Understanding SHAP Values: From Theory to Implementation
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <blockquote>
<p><strong>TL;DR</strong> — SHAP (SHapley Additive exPlanations) is a principled framework for interpreting ML predictions. By borrowing ideas from cooperative game theory, SHAP quantifies the contribution of each feature for a given prediction. In this guide, we’ll unpack the math, intuition, and practical implementation.</p>
</blockquote>
<p><img src="pizza_shapley_story.gif" alt="Pizza Shapley Animation"></p>
<hr>
<h2 id="1-motivation--why-interpretability-matters">1. Motivation — Why Interpretability Matters</h2>
<p>Modern machine learning models are increasingly complex, think ensembles, gradient boosting, or deep neural networks. Without interpretability, these models are <strong>black boxes</strong>. Understanding <em>why</em> a model makes a prediction is essential for:</p>
        </div>
        
          <p class="f6 lh-copy mv0">By Alejandro Paredes</p><a href="/posts/shapely-values/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
      </div>
    </div>
  </div>
</article>

          </div>
        
          <div class="w-100 mb4 relative">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l dark-gray no-underline">
    <div class="flex-column flex-row-ns flex">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/posts/unmaskingai/" class="color-inherit dim link">
            
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <!-- ---
title: "Unmasking AI: Technical Insights and Policy Implications"
date: 2025-11-16
draft: false
description: "A deep dive into Joy Buolamwini’s work on AI bias, datasets, and policy"
tags: ["AI", "explainable-ai", "fairness", "machine-learning", "algorithmic-bias"]
categories: ["AI & ML", "Explainable AI"]
author: "Alejandro Paredes"
math: true
---

```

# Understanding Bias in AI: A Technical Exploration

Joy Buolamwini’s *Unmasking AI* illustrates the intersection of machine learning, human-centered design, and policy. This post highlights the book’s key technical lessons and their implications for AI governance. Interactive elements allow readers to engage with concepts like dataset composition, classification errors, and intersectional performance disparities.

---

## Interactive Chapter Guide

Click to expand technical and policy insights from each chapter.

<details>
<summary>Chapter 1–3: Foundations and Early Observations</summary>

**Technical Takeaways**:

* Early exposure to computing, graphics, and human-centered design.
* Experiments with interactive installations revealed bias in computer vision systems (e.g., Upbeat Walls failing for darker faces).

**Policy Insight**:

* Even early-stage tech environments can embed exclusionary practices.
* Raises awareness that human-centered AI design must consider diverse populations.

</details>

<details>
<summary>Chapter 4–6: Public Demonstrations and Face Analytics</summary>

**Technical Takeaways**:

* Aspire Mirror exhibit demonstrates face-detection failures.
* Defines “face analytics” tasks: detection, classification, recognition.
* Highlights “AI functionality fallacy”: overestimating what algorithms can do.

**Interactive Python Example: Simulating Detection Bias**

```python
import matplotlib.pyplot as plt
import numpy as np

# Simulate misclassification probabilities
skin_tones = ['Light', 'Medium', 'Dark']
genders = ['Male', 'Female']
error_rates = np.array([[0.02, 0.10], [0.05, 0.20], [0.08, 0.30]])  # Dark female highest

fig, ax = plt.subplots()
im = ax.imshow(error_rates, cmap='Reds')

# Show labels
ax.set_xticks(np.arange(len(genders)))
ax.set_yticks(np.arange(len(skin_tones)))
ax.set_xticklabels(genders)
ax.set_yticklabels(skin_tones)
ax.set_title("Simulated Face Recognition Error Rates")
for i in range(len(skin_tones)):
    for j in range(len(genders)):
        ax.text(j, i, f"{error_rates[i, j]*100:.0f}%", ha="center", va="center", color="black")
plt.show()
```

This plot mirrors Buolamwini’s findings: intersectional analysis is critical to uncover hidden bias.

**Policy Insight**:

* Public demonstrations translate technical results into societal impact.
* Sets groundwork for calls for transparency and auditing of AI systems.

</details>

<details>
<summary>Chapter 7–9: Datasets, Ground Truth, and Bias Measurement</summary>

**Technical Takeaways**:

* Dataset composition dramatically affects model performance.
* “Ground truth” labels are subjective, reflecting social constructs.
* Intersectional analysis (race × gender) required to reveal real disparities.

**Interactive Example: Ground Truth Simulation**

```python
import seaborn as sns

# Fake dataset label distribution
labels = ['Male', 'Female']
skin = ['Light', 'Dark']
np.random.seed(42)
data = np.random.choice(labels, size=2000, p=[0.7,0.3])
sns.histplot(data, hue=np.random.choice(skin, size=2000), multiple='stack')
plt.title("Simulated Dataset Label Distribution")
plt.show()
```

Shows how skewed datasets (e.g., 70% male) bias learning outcomes.

**Policy Insight**:

* Labeling decisions embed power structures.
* Transparency in datasets is a prerequisite for fair AI governance.

</details>

<details>
<summary>Chapter 10–13: Gender Shades and Public Advocacy</summary>

**Technical Takeaways**:

* Commercial APIs exhibit higher errors for dark-skinned women.
* Intersectional error rates: critical metric beyond average accuracy.

**Interactive Example: Gender Shades Table**

| API       | Light Male | Light Female | Dark Male | Dark Female |
| --------- | ---------- | ------------ | --------- | ----------- |
| Microsoft | 97.4%      | 89.3%        | 96.5%     | 78.7%       |
| IBM       | 99.3%      | 87.5%        | 98.0%     | 69.7%       |
| Face++    | 99.3%      | 78.7%        | 96.0%     | 65.3%       |

**Policy Insight**:

* Independent audits enforce accountability.
* Results shaped public debates and prompted corporate retraining of models.

</details>

<details>
<summary>Chapter 14–23: Advocacy, AI Bill of Rights, and Policy Impact</summary>

**Technical Takeaways**:

* AI systems in real-world contexts continue to fail without diverse data.
* Creative communication (poetry, demonstrations) helps explain technical issues.

**Policy Insight**:

* Grassroots activism and legislative engagement both required for systemic change.
* Buolamwini’s work led to AI Bill of Rights principles, emphasizing fairness, transparency, and consent.
* Intersectional analysis informs policy decisions on biometric data and automated systems.

</details>

---

## Key Takeaways

* **Data is Destiny**: Models inherit biases from datasets; technical rigor must pair with social awareness.
* **Intersectional Analysis**: Evaluating models across multiple demographic axes reveals disparities hidden in averages.
* **Advocacy Meets Technical Insight**: Combining coding experiments, public demos, and performance audits amplifies impact.
* **Policy Implications**: AI fairness requires regulation, transparent audits, and stakeholder involvement at every stage.

---

## Explore Yourself

Try modifying the Python plots above with different error rates or label distributions to see how dataset composition affects AI fairness. Use these experiments as a guide for your own exploratory projects in explainable AI. -->
        </div>
        <a href="/posts/unmaskingai/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
      </div>
    </div>
  </div>
</article>

          </div>
        
      </section>

      

    </div>
  

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://localhost:1313/" >
    &copy;  Alejandro Paredes La Torre 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
