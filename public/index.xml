<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alejandro Paredes La Torre</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Alejandro Paredes La Torre</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Applying Concepts from A Hacker&#39;s Mind by Bruce Schneier</title>
      <link>http://localhost:1313/posts/ahackersmind/</link>
      <pubDate>Sun, 16 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/ahackersmind/</guid>
      <description>&lt;h1 id=&#34;a-hackers-mind-real-world-applied-insights&#34;&gt;A Hacker&amp;rsquo;s Mind: Real World Applied Insights&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;A Hacker&amp;rsquo;s Mind&lt;/strong&gt; analyses how hacking transcends code, networks, and cryptographic systems. Schneier frames hacking as exploiting rules, incentives, and institutional structures. This post examines direct applications from the book through modern examples and thought experiments.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;core-premise-illustrated&#34;&gt;Core Premise Illustrated&lt;/h2&gt;&#xA;&lt;p&gt;Rules are not objective barriers. They are design artifacts. Anyone with incentive, creativity, and domain insight can search for unintended outcomes inside procedural, legal, economic, or algorithmic frameworks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding SHAP Values: From Theory to Implementation</title>
      <link>http://localhost:1313/posts/shapely-values/</link>
      <pubDate>Sun, 19 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/shapely-values/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; — SHAP (SHapley Additive exPlanations) is a principled framework for interpreting ML predictions. By borrowing ideas from cooperative game theory, SHAP quantifies the contribution of each feature for a given prediction. In this guide, we’ll unpack the math, intuition, and practical implementation.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;img src=&#34;pizza_shapley_story.gif&#34; alt=&#34;Pizza Shapley Animation&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-motivation--why-interpretability-matters&#34;&gt;1. Motivation — Why Interpretability Matters&lt;/h2&gt;&#xA;&lt;p&gt;Modern machine learning models are increasingly complex, think ensembles, gradient boosting, or deep neural networks. Without interpretability, these models are &lt;strong&gt;black boxes&lt;/strong&gt;. Understanding &lt;em&gt;why&lt;/em&gt; a model makes a prediction is essential for:&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/posts/unmaskingai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/unmaskingai/</guid>
      <description>&lt;!-- ---&#xD;&#xA;title: &#34;Unmasking AI: Technical Insights and Policy Implications&#34;&#xD;&#xA;date: 2025-11-16&#xD;&#xA;draft: false&#xD;&#xA;description: &#34;A deep dive into Joy Buolamwini’s work on AI bias, datasets, and policy&#34;&#xD;&#xA;tags: [&#34;AI&#34;, &#34;explainable-ai&#34;, &#34;fairness&#34;, &#34;machine-learning&#34;, &#34;algorithmic-bias&#34;]&#xD;&#xA;categories: [&#34;AI &amp; ML&#34;, &#34;Explainable AI&#34;]&#xD;&#xA;author: &#34;Alejandro Paredes&#34;&#xD;&#xA;math: true&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;# Understanding Bias in AI: A Technical Exploration&#xD;&#xA;&#xD;&#xA;Joy Buolamwini’s *Unmasking AI* illustrates the intersection of machine learning, human-centered design, and policy. This post highlights the book’s key technical lessons and their implications for AI governance. Interactive elements allow readers to engage with concepts like dataset composition, classification errors, and intersectional performance disparities.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;## Interactive Chapter Guide&#xD;&#xA;&#xD;&#xA;Click to expand technical and policy insights from each chapter.&#xD;&#xA;&#xD;&#xA;&lt;details&gt;&#xD;&#xA;&lt;summary&gt;Chapter 1–3: Foundations and Early Observations&lt;/summary&gt;&#xD;&#xA;&#xD;&#xA;**Technical Takeaways**:&#xD;&#xA;&#xD;&#xA;* Early exposure to computing, graphics, and human-centered design.&#xD;&#xA;* Experiments with interactive installations revealed bias in computer vision systems (e.g., Upbeat Walls failing for darker faces).&#xD;&#xA;&#xD;&#xA;**Policy Insight**:&#xD;&#xA;&#xD;&#xA;* Even early-stage tech environments can embed exclusionary practices.&#xD;&#xA;* Raises awareness that human-centered AI design must consider diverse populations.&#xD;&#xA;&#xD;&#xA;&lt;/details&gt;&#xD;&#xA;&#xD;&#xA;&lt;details&gt;&#xD;&#xA;&lt;summary&gt;Chapter 4–6: Public Demonstrations and Face Analytics&lt;/summary&gt;&#xD;&#xA;&#xD;&#xA;**Technical Takeaways**:&#xD;&#xA;&#xD;&#xA;* Aspire Mirror exhibit demonstrates face-detection failures.&#xD;&#xA;* Defines “face analytics” tasks: detection, classification, recognition.&#xD;&#xA;* Highlights “AI functionality fallacy”: overestimating what algorithms can do.&#xD;&#xA;&#xD;&#xA;**Interactive Python Example: Simulating Detection Bias**&#xD;&#xA;&#xD;&#xA;```python&#xD;&#xA;import matplotlib.pyplot as plt&#xD;&#xA;import numpy as np&#xD;&#xA;&#xD;&#xA;# Simulate misclassification probabilities&#xD;&#xA;skin_tones = [&#39;Light&#39;, &#39;Medium&#39;, &#39;Dark&#39;]&#xD;&#xA;genders = [&#39;Male&#39;, &#39;Female&#39;]&#xD;&#xA;error_rates = np.array([[0.02, 0.10], [0.05, 0.20], [0.08, 0.30]])  # Dark female highest&#xD;&#xA;&#xD;&#xA;fig, ax = plt.subplots()&#xD;&#xA;im = ax.imshow(error_rates, cmap=&#39;Reds&#39;)&#xD;&#xA;&#xD;&#xA;# Show labels&#xD;&#xA;ax.set_xticks(np.arange(len(genders)))&#xD;&#xA;ax.set_yticks(np.arange(len(skin_tones)))&#xD;&#xA;ax.set_xticklabels(genders)&#xD;&#xA;ax.set_yticklabels(skin_tones)&#xD;&#xA;ax.set_title(&#34;Simulated Face Recognition Error Rates&#34;)&#xD;&#xA;for i in range(len(skin_tones)):&#xD;&#xA;    for j in range(len(genders)):&#xD;&#xA;        ax.text(j, i, f&#34;{error_rates[i, j]*100:.0f}%&#34;, ha=&#34;center&#34;, va=&#34;center&#34;, color=&#34;black&#34;)&#xD;&#xA;plt.show()&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;This plot mirrors Buolamwini’s findings: intersectional analysis is critical to uncover hidden bias.&#xD;&#xA;&#xD;&#xA;**Policy Insight**:&#xD;&#xA;&#xD;&#xA;* Public demonstrations translate technical results into societal impact.&#xD;&#xA;* Sets groundwork for calls for transparency and auditing of AI systems.&#xD;&#xA;&#xD;&#xA;&lt;/details&gt;&#xD;&#xA;&#xD;&#xA;&lt;details&gt;&#xD;&#xA;&lt;summary&gt;Chapter 7–9: Datasets, Ground Truth, and Bias Measurement&lt;/summary&gt;&#xD;&#xA;&#xD;&#xA;**Technical Takeaways**:&#xD;&#xA;&#xD;&#xA;* Dataset composition dramatically affects model performance.&#xD;&#xA;* “Ground truth” labels are subjective, reflecting social constructs.&#xD;&#xA;* Intersectional analysis (race × gender) required to reveal real disparities.&#xD;&#xA;&#xD;&#xA;**Interactive Example: Ground Truth Simulation**&#xD;&#xA;&#xD;&#xA;```python&#xD;&#xA;import seaborn as sns&#xD;&#xA;&#xD;&#xA;# Fake dataset label distribution&#xD;&#xA;labels = [&#39;Male&#39;, &#39;Female&#39;]&#xD;&#xA;skin = [&#39;Light&#39;, &#39;Dark&#39;]&#xD;&#xA;np.random.seed(42)&#xD;&#xA;data = np.random.choice(labels, size=2000, p=[0.7,0.3])&#xD;&#xA;sns.histplot(data, hue=np.random.choice(skin, size=2000), multiple=&#39;stack&#39;)&#xD;&#xA;plt.title(&#34;Simulated Dataset Label Distribution&#34;)&#xD;&#xA;plt.show()&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;Shows how skewed datasets (e.g., 70% male) bias learning outcomes.&#xD;&#xA;&#xD;&#xA;**Policy Insight**:&#xD;&#xA;&#xD;&#xA;* Labeling decisions embed power structures.&#xD;&#xA;* Transparency in datasets is a prerequisite for fair AI governance.&#xD;&#xA;&#xD;&#xA;&lt;/details&gt;&#xD;&#xA;&#xD;&#xA;&lt;details&gt;&#xD;&#xA;&lt;summary&gt;Chapter 10–13: Gender Shades and Public Advocacy&lt;/summary&gt;&#xD;&#xA;&#xD;&#xA;**Technical Takeaways**:&#xD;&#xA;&#xD;&#xA;* Commercial APIs exhibit higher errors for dark-skinned women.&#xD;&#xA;* Intersectional error rates: critical metric beyond average accuracy.&#xD;&#xA;&#xD;&#xA;**Interactive Example: Gender Shades Table**&#xD;&#xA;&#xD;&#xA;| API       | Light Male | Light Female | Dark Male | Dark Female |&#xD;&#xA;| --------- | ---------- | ------------ | --------- | ----------- |&#xD;&#xA;| Microsoft | 97.4%      | 89.3%        | 96.5%     | 78.7%       |&#xD;&#xA;| IBM       | 99.3%      | 87.5%        | 98.0%     | 69.7%       |&#xD;&#xA;| Face++    | 99.3%      | 78.7%        | 96.0%     | 65.3%       |&#xD;&#xA;&#xD;&#xA;**Policy Insight**:&#xD;&#xA;&#xD;&#xA;* Independent audits enforce accountability.&#xD;&#xA;* Results shaped public debates and prompted corporate retraining of models.&#xD;&#xA;&#xD;&#xA;&lt;/details&gt;&#xD;&#xA;&#xD;&#xA;&lt;details&gt;&#xD;&#xA;&lt;summary&gt;Chapter 14–23: Advocacy, AI Bill of Rights, and Policy Impact&lt;/summary&gt;&#xD;&#xA;&#xD;&#xA;**Technical Takeaways**:&#xD;&#xA;&#xD;&#xA;* AI systems in real-world contexts continue to fail without diverse data.&#xD;&#xA;* Creative communication (poetry, demonstrations) helps explain technical issues.&#xD;&#xA;&#xD;&#xA;**Policy Insight**:&#xD;&#xA;&#xD;&#xA;* Grassroots activism and legislative engagement both required for systemic change.&#xD;&#xA;* Buolamwini’s work led to AI Bill of Rights principles, emphasizing fairness, transparency, and consent.&#xD;&#xA;* Intersectional analysis informs policy decisions on biometric data and automated systems.&#xD;&#xA;&#xD;&#xA;&lt;/details&gt;&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;## Key Takeaways&#xD;&#xA;&#xD;&#xA;* **Data is Destiny**: Models inherit biases from datasets; technical rigor must pair with social awareness.&#xD;&#xA;* **Intersectional Analysis**: Evaluating models across multiple demographic axes reveals disparities hidden in averages.&#xD;&#xA;* **Advocacy Meets Technical Insight**: Combining coding experiments, public demos, and performance audits amplifies impact.&#xD;&#xA;* **Policy Implications**: AI fairness requires regulation, transparent audits, and stakeholder involvement at every stage.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;## Explore Yourself&#xD;&#xA;&#xD;&#xA;Try modifying the Python plots above with different error rates or label distributions to see how dataset composition affects AI fairness. Use these experiments as a guide for your own exploratory projects in explainable AI. --&gt;</description>
    </item>
  </channel>
</rss>
