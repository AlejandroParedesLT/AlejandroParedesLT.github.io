<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>

<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
    
    
    <title>Alejandro Paredes La Torre</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="">
    <meta name="generator" content="Hugo 0.151.2">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >




    


    
      

    

    
    
      <link href="/categories/index.xml" rel="alternate" type="application/rss+xml" title="Alejandro Paredes La Torre" />
      <link href="/categories/index.xml" rel="feed" type="application/rss+xml" title="Alejandro Paredes La Torre" />
      
    

    
      <link rel="canonical" href="http://localhost:1313/categories/">
    

    <meta property="og:url" content="http://localhost:1313/categories/">
  <meta property="og:site_name" content="Alejandro Paredes La Torre">
  <meta property="og:title" content="Categories">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="website">

  <meta itemprop="name" content="Categories">
  <meta itemprop="datePublished" content="2025-11-16T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-11-16T00:00:00+00:00">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Categories">

	
  </head><body class="ma0 avenir bg-near-white development">

    

  <header>
    <div class="pb3-m pb6-l bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Alejandro Paredes La Torre
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv3 ph3 ph4-ns">
        <h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">
          Categories
        </h1>
        
      </div>
    </div>
  </header>


    <main class="pb7" role="main">
      
    
  <article class="cf pa3 pa4-m pa4-l">
    <div class="measure-wide-l center f4 lh-copy nested-copy-line-height nested-links mid-gray">
      
    </div>
  </article>
  <div class="mw8 center">
    <section class="ph4">
      
        <h2 class="f1">
          <a href="/categories/ai--ml/" class="link blue hover-black">
            Category: AI &amp; ML
          </a>
        </h2>
        
          <div class="w-100 mb4 nested-copy-line-height relative bg-white">
  <div class="mb3 pa4 gray overflow-hidden bg-white">
    <span class="f6 db">Posts</span>
    <h1 class="f3 near-black">
      <a href="/posts/unmaskingai/" class="link black dim">
        Unmasking AI: Technical Insights and Policy Implications
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <pre tabindex="0"><code>
# Understanding Bias in AI: A Technical Exploration

Joy Buolamwini’s *Unmasking AI* illustrates the intersection of machine learning, human-centered design, and policy. This post highlights the book’s key technical lessons and their implications for AI governance. Interactive elements allow readers to engage with concepts like dataset composition, classification errors, and intersectional performance disparities.

---

## Interactive Chapter Guide

Click to expand technical and policy insights from each chapter.

&lt;details&gt;
&lt;summary&gt;Chapter 1–3: Foundations and Early Observations&lt;/summary&gt;

**Technical Takeaways**:

* Early exposure to computing, graphics, and human-centered design.
* Experiments with interactive installations revealed bias in computer vision systems (e.g., Upbeat Walls failing for darker faces).

**Policy Insight**:

* Even early-stage tech environments can embed exclusionary practices.
* Raises awareness that human-centered AI design must consider diverse populations.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Chapter 4–6: Public Demonstrations and Face Analytics&lt;/summary&gt;

**Technical Takeaways**:

* Aspire Mirror exhibit demonstrates face-detection failures.
* Defines “face analytics” tasks: detection, classification, recognition.
* Highlights “AI functionality fallacy”: overestimating what algorithms can do.

**Interactive Python Example: Simulating Detection Bias**

```python
import matplotlib.pyplot as plt
import numpy as np

# Simulate misclassification probabilities
skin_tones = [&#39;Light&#39;, &#39;Medium&#39;, &#39;Dark&#39;]
genders = [&#39;Male&#39;, &#39;Female&#39;]
error_rates = np.array([[0.02, 0.10], [0.05, 0.20], [0.08, 0.30]])  # Dark female highest

fig, ax = plt.subplots()
im = ax.imshow(error_rates, cmap=&#39;Reds&#39;)

# Show labels
ax.set_xticks(np.arange(len(genders)))
ax.set_yticks(np.arange(len(skin_tones)))
ax.set_xticklabels(genders)
ax.set_yticklabels(skin_tones)
ax.set_title(&#34;Simulated Face Recognition Error Rates&#34;)
for i in range(len(skin_tones)):
    for j in range(len(genders)):
        ax.text(j, i, f&#34;{error_rates[i, j]*100:.0f}%&#34;, ha=&#34;center&#34;, va=&#34;center&#34;, color=&#34;black&#34;)
plt.show()
</code></pre><p>This plot mirrors Buolamwini’s findings: intersectional analysis is critical to uncover hidden bias.</p>
    </div>
    <a href="/posts/unmaskingai/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>
</div>

        
          <div class="w-100 mb4 nested-copy-line-height relative bg-white">
  <div class="mb3 pa4 gray overflow-hidden bg-white">
    <span class="f6 db">Posts</span>
    <h1 class="f3 near-black">
      <a href="/posts/shapely-values/" class="link black dim">
        Understanding SHAP Values: From Theory to Implementation
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <blockquote>
<p><strong>TL;DR</strong> — SHAP (SHapley Additive exPlanations) is a principled framework for interpreting ML predictions. By borrowing ideas from cooperative game theory, SHAP quantifies the contribution of each feature for a given prediction. In this guide, we’ll unpack the math, intuition, and practical implementation.</p>
</blockquote>
<p><img src="pizza_shapley_story.gif" alt="Pizza Shapley Animation"></p>
<hr>
<h2 id="1-motivation--why-interpretability-matters">1. Motivation — Why Interpretability Matters</h2>
<p>Modern machine learning models are increasingly complex, think ensembles, gradient boosting, or deep neural networks. Without interpretability, these models are <strong>black boxes</strong>. Understanding <em>why</em> a model makes a prediction is essential for:</p>
    </div>
    <a href="/posts/shapely-values/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>
</div>

        
      
        <h2 class="f1">
          <a href="/categories/explainable-ai/" class="link blue hover-black">
            Category: Explainable AI
          </a>
        </h2>
        
          <div class="w-100 mb4 nested-copy-line-height relative bg-white">
  <div class="mb3 pa4 gray overflow-hidden bg-white">
    <span class="f6 db">Posts</span>
    <h1 class="f3 near-black">
      <a href="/posts/unmaskingai/" class="link black dim">
        Unmasking AI: Technical Insights and Policy Implications
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <pre tabindex="0"><code>
# Understanding Bias in AI: A Technical Exploration

Joy Buolamwini’s *Unmasking AI* illustrates the intersection of machine learning, human-centered design, and policy. This post highlights the book’s key technical lessons and their implications for AI governance. Interactive elements allow readers to engage with concepts like dataset composition, classification errors, and intersectional performance disparities.

---

## Interactive Chapter Guide

Click to expand technical and policy insights from each chapter.

&lt;details&gt;
&lt;summary&gt;Chapter 1–3: Foundations and Early Observations&lt;/summary&gt;

**Technical Takeaways**:

* Early exposure to computing, graphics, and human-centered design.
* Experiments with interactive installations revealed bias in computer vision systems (e.g., Upbeat Walls failing for darker faces).

**Policy Insight**:

* Even early-stage tech environments can embed exclusionary practices.
* Raises awareness that human-centered AI design must consider diverse populations.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Chapter 4–6: Public Demonstrations and Face Analytics&lt;/summary&gt;

**Technical Takeaways**:

* Aspire Mirror exhibit demonstrates face-detection failures.
* Defines “face analytics” tasks: detection, classification, recognition.
* Highlights “AI functionality fallacy”: overestimating what algorithms can do.

**Interactive Python Example: Simulating Detection Bias**

```python
import matplotlib.pyplot as plt
import numpy as np

# Simulate misclassification probabilities
skin_tones = [&#39;Light&#39;, &#39;Medium&#39;, &#39;Dark&#39;]
genders = [&#39;Male&#39;, &#39;Female&#39;]
error_rates = np.array([[0.02, 0.10], [0.05, 0.20], [0.08, 0.30]])  # Dark female highest

fig, ax = plt.subplots()
im = ax.imshow(error_rates, cmap=&#39;Reds&#39;)

# Show labels
ax.set_xticks(np.arange(len(genders)))
ax.set_yticks(np.arange(len(skin_tones)))
ax.set_xticklabels(genders)
ax.set_yticklabels(skin_tones)
ax.set_title(&#34;Simulated Face Recognition Error Rates&#34;)
for i in range(len(skin_tones)):
    for j in range(len(genders)):
        ax.text(j, i, f&#34;{error_rates[i, j]*100:.0f}%&#34;, ha=&#34;center&#34;, va=&#34;center&#34;, color=&#34;black&#34;)
plt.show()
</code></pre><p>This plot mirrors Buolamwini’s findings: intersectional analysis is critical to uncover hidden bias.</p>
    </div>
    <a href="/posts/unmaskingai/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>
</div>

        
          <div class="w-100 mb4 nested-copy-line-height relative bg-white">
  <div class="mb3 pa4 gray overflow-hidden bg-white">
    <span class="f6 db">Posts</span>
    <h1 class="f3 near-black">
      <a href="/posts/shapely-values/" class="link black dim">
        Understanding SHAP Values: From Theory to Implementation
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <blockquote>
<p><strong>TL;DR</strong> — SHAP (SHapley Additive exPlanations) is a principled framework for interpreting ML predictions. By borrowing ideas from cooperative game theory, SHAP quantifies the contribution of each feature for a given prediction. In this guide, we’ll unpack the math, intuition, and practical implementation.</p>
</blockquote>
<p><img src="pizza_shapley_story.gif" alt="Pizza Shapley Animation"></p>
<hr>
<h2 id="1-motivation--why-interpretability-matters">1. Motivation — Why Interpretability Matters</h2>
<p>Modern machine learning models are increasingly complex, think ensembles, gradient boosting, or deep neural networks. Without interpretability, these models are <strong>black boxes</strong>. Understanding <em>why</em> a model makes a prediction is essential for:</p>
    </div>
    <a href="/posts/shapely-values/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>
</div>

        
      
    </section>
  </div>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://localhost:1313/" >
    &copy;  Alejandro Paredes La Torre 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
